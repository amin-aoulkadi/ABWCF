#
# Default configuration of the ABWCF.
# Use an application.conf file to override these default values.
#
# Syntax: https://github.com/lightbend/config/blob/main/HOCON.md
#

abwcf {
  actors {
    crawl-depth-limiter {
      # The maximum depth to crawl to, relative to the seed URLs. Seed URLs have a crawl depth of zero.
      # If this field is set to zero, only seed URLs are processed.
      max-crawl-depth = 2147483647 # Int.MaxValue
    }

    fetcher-manager {
      # Delay before the first managment action after the FetcherManager actor was spawned.
      initial-delay = "5 seconds"

      # Time between management actions.
      management-delay = "10 seconds"
    }

    host-queue {
      # The receive timeout for passivation. A HostQueue actor is only eligible for passivation if its queue is empty.
      passivation-receive-timeout = "10 seconds"
    }

    page-manager {
      # The receive timeout for passivation. PageManager actors are always eligible for passivation.
      passivation-receive-timeout = "2 minutes"
    }

    page-restorer {
      # Delay before the first restore attempt after the PageRestorer actor was spawned.
      initial-delay = "3 seconds"

      # Time between restore attempts.
      restore-delay = "1 minute"
    }

    robots-fetcher {
      # The maximum accepted content length (in bytes) per robots.txt file.
      # If a robots.txt file exceeds this length, it is truncated.
      # The Robots Exclusion Protocol (RFC 9309) specifies a minimum value of 500 kibibytes.
      max-content-length = "500 KiB"

      # The maximum number of consecutive redirections per robots.txt file.
      # If the robots.txt file is not reached after this number of redirections, the file is treated as unavailable.
      # The Robots Exclusion Protocol specifies a minimum value of five.
      max-redirects = 5

      # The bandwidth budget per robots.txt file.
      bandwidth-budget-per-file = "500 kB"
    }

    url-filter {
      # URLs that are longer than this are not crawled.
      max-url-length = 2048

      # URLs must match at least one of these regular expressions to be crawled.
      # Use triple-quoted strings (e.g. """https?://example\.com/.*""") to avoid having to escape certain characters.
      # Regex flavor: Java (see https://docs.oracle.com/en/java/javase/23/docs/api/java.base/java/util/regex/Pattern.html)
      must-match = [".*"]

      # URLs must not match any of these regular expressions to be crawled.
      must-not-match = []
    }

    url-normalizer {
      # Remove the user information component from URLs (e.g. "https://user@example.com/").
      remove-userinfo = true

      # Remove the query component from URLs (e.g. "https://example.com/?key=value").
      remove-query = false

      # Remove the fragment component from URLs (e.g. "https://example.com/#fragment").
      remove-fragment = true
    }

    url-supplier {
      # The timeout for an individual request to the HostQueueRouter actor.
      ask-timeout = "3 seconds"

      # The number of requests in a request burst.
      burst-length = 10

      # The minimum delay (in milliseconds) between request bursts.
      min-delay = 10

      # The maximum delay (in milliseconds) between request bursts.
      max-delay = 1000
    }
  }

  fetching {
    # The maximum accepted content length (in bytes) per HTTP response.
    max-content-length = "1 MB"

    # The maximum bandwidth available for fetching on this node.
    # The bandwidth is distributed evenly among the Fetcher actors on this node. Other network traffic (e.g. database connections) is not subject to this bandwidth limit.
    # This field expects a number of bytes. It is always assumed that this is a number of bytes per second.
    total-bandwidth-budget = "5 MB"

    # The minimum bandwidth budget per Fetcher actor.
    # Within the total bandwidth budget, Fetcher actors may be allocated more than the minimum bandwidth, but not less. The number of concurrent Fetcher actors on this node is therefore limited to:
    #   b / m
    # b = total bandwidth budget
    # m = minimum bandwidth budget per Fetcher actor
    min-bandwidth-budget-per-fetcher = "200 kB"
  }

  persistence {
    # Slick database configuration.
    # Override this field to use a database that is supported by Slick for persistence.
    slick-config = null
  }

  robots {
    # The default crawl delay (i.e. time between requests to the same host) to be used if no crawl delay is specified by the robots.txt file.
    default-crawl-delay = "1 second"

    # The minimum accepted crawl delay.
    # If a robots.txt file specifies a shorter crawl delay, the delay from this field is used.
    min-crawl-delay = "0 seconds"

    # The maximum accepted crawl delay.
    # If a robots.txt file specifies a longer crawl delay, the delay from this field is used.
    max-crawl-delay = "20 seconds"

    # Cache lifetime for rules from a robots.txt file.
    # When the cached rules expire, the robots.txt file is fetched again (if the host is still being crawled).
    # The Robots Exclusion Protocol specifies a maximum value of 24 hours.
    valid-rules-lifetime = "24 hours"

    # Cache lifetime for the default rules that are used if a robots.txt file is unavailable as defined by the Robots Exclusion Protocol (e.g. if the server responds with a 4xx status code).
    unavailable-rules-lifetime = "24 hours"

    # Cache lifetime for the default rules that are used if a robots.txt file is unreachable as defined by the Robots Exclusion Protocol (e.g. if the server responds with a 5xx status code).
    unreachable-rules-lifetime = "1 hour"
  }
}
