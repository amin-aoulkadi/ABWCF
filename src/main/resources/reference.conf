#
# Default configuration of the ABWCF.
# Use an application.conf file to override these default values.
#
# Syntax: https://github.com/lightbend/config/blob/main/HOCON.md
#

abwcf {
  actors {
    cluster-node-metrics-collector {
      # Time between metric collection iterations.
      # This field does not set the export interval of the OpenTelemetry SDK or Java agent.
      collection-delay = "30 seconds"
    }

    crawl-depth-limiter {
      # The maximum depth to crawl to, relative to the seed URLs. Seed URLs have a crawl depth of zero.
      # If this field is set to zero, only seed URLs are processed.
      max-crawl-depth = 2147483647 # Int.MaxValue
    }
    
    deduplicator {
      # The maximum size of the URL cache.
      # There is one cache entry per unique URL.
      max-cache-size = 2500
    }

    fetcher-manager {
      # Delay before the first managment action after the FetcherManager actor was spawned.
      initial-delay = "5 seconds"

      # Time between management actions.
      management-delay = "10 seconds"
    }

    host-manager {
      # The receive timeout for passivation.
      passivation-receive-timeout = "2 minutes"
    }

    host-queue {
      # The receive timeout for passivation. A HostQueue actor is only eligible for passivation if its queue is empty.
      passivation-receive-timeout = "10 seconds"
    }

    lenient-robots-filter {
      # The timeout for an individual request to a HostManager actor.
      ask-timeout = "10 seconds"

      # The duration for which URLs are sent downstream unfiltered if the required host information is currently unavailable (e.g. if the corresponding robots.txt file has not been fetched yet).
      # When the duration ends, a new request for the required host information is sent to the HostManager actor.
      fail-open-duration = "5 minutes"

      # The maximum size of the host information cache (which caches rules from robots.txt files).
      # There is one cache entry per host. That cache entry contains all rules for the corresponding host.
      max-cache-size = 1000
    }

    page-manager {
      # The receive timeout for passivation.
      passivation-receive-timeout = "2 minutes"
    }

    page-restorer {
      # Delay before the first restore attempt after the PageRestorer actor was spawned.
      initial-delay = "3 seconds"

      # Time between restore attempts.
      restore-delay = "1 minute"
    }

    strict-robots-filter {
      # The timeout for an individual request to a HostManager actor.
      ask-timeout = "10 seconds"

      # The duration for which pages are ignored if the required host information is currently unavailable (e.g. if the corresponding robots.txt file has not been fetched yet).
      # When the duration ends, a new request for the required host information is sent to the HostManager actor.
      fail-close-duration = "2 minutes"

      # The maximum size of the host information cache (which caches rules from robots.txt files).
      # There is one cache entry per host. That cache entry contains all rules for the corresponding host.
      max-cache-size = 1000
    }

    url-filter {
      # URLs that are longer than this are not crawled.
      max-url-length = 2048

      # URLs must match at least one of these regular expressions to be crawled.
      # Use triple-quoted strings (e.g. """https?://example\.com/.*""") to avoid having to escape certain characters.
      # Regex flavor: Java (see https://docs.oracle.com/en/java/javase/23/docs/api/java.base/java/util/regex/Pattern.html)
      must-match = [".*"]

      # URLs must not match any of these regular expressions to be crawled.
      must-not-match = []
    }

    url-normalizer {
      # Log exceptions that occur while normalizing URLs.
      log-exceptions = true

      # Remove the user information component from URLs (e.g. "https://user@example.com/").
      remove-userinfo = true

      # Remove the query component from URLs (e.g. "https://example.com/?key=value").
      remove-query = false

      # Remove the fragment component from URLs (e.g. "https://example.com/#fragment").
      remove-fragment = true
    }

    url-supplier {
      # The timeout for an individual request to the HostQueueRouter actor.
      ask-timeout = "3 seconds"

      # The number of requests in a request burst.
      burst-length = 10

      # The minimum delay (in milliseconds) between request bursts.
      min-delay = 10

      # The maximum delay (in milliseconds) between request bursts.
      max-delay = 1000
    }
  }

  fetching {
    # The maximum accepted content length (in bytes) per HTTP response.
    max-content-length = "1 MB"

    # The maximum bandwidth available for fetching on this node.
    # The bandwidth is distributed evenly among the Fetcher actors on this node. Other network traffic (e.g. database connections) is not subject to this bandwidth limit.
    # This field expects a number of bytes. It is always assumed that this is a number of bytes per second.
    total-bandwidth-budget = "5 MB"

    # The minimum bandwidth budget per Fetcher actor.
    # Within the total bandwidth budget, Fetcher actors may be allocated more than the minimum bandwidth, but not less. The number of concurrent Fetcher actors on this node is therefore limited to:
    #   b / m
    # b = total bandwidth budget
    # m = minimum bandwidth budget per Fetcher actor
    min-bandwidth-budget-per-fetcher = "200 kB"
  }

  persistence {
    slick {
      # Slick database configuration.
      # Override this field to use a database that is supported by Slick for persistence.
      database = null

      host {
        insert {
          # The maximum number of host information entries to insert in one batch.
          max-batch-size = 5

          # The maximum amount of time (after buffering the first batch element) to wait for more batch elements before sending the batch to the database.
          max-batch-delay = "350 milliseconds"
        }

        select {
          # The maximum number of active recovery queries.
          # Under the hood, database communication is handled by a Java Executor with a size-limited task queue. To avoid completely filling the executor's task queue with recovery queries, the HostReader actor limits the number of active recovery queries and buffers excess recovery queries in its own unbounded queue.
          max-active-recovery-queries = 15
        }

        update {
          # The maximum number of host information entries to update in one batch.
          max-batch-size = 5

          # The maximum amount of time (after buffering the first batch element) to wait for more batch elements before sending the batch to the database.
          max-batch-delay = "350 milliseconds"
        }
      }

      page {
        insert {
          # The maximum number of pages to insert in one batch.
          max-batch-size = 1000

          # The maximum amount of time (after buffering the first batch element) to wait for more batch elements before sending the batch to the database.
          max-batch-delay = "500 milliseconds"
        }

        select {
          # The maximum number of active recovery queries.
          # Under the hood, database communication is handled by a Java Executor with a size-limited task queue. To avoid completely filling the executor's task queue with recovery queries, the PageReader actor limits the number of active recovery queries and buffers excess recovery queries in its own unbounded queue.
          max-active-recovery-queries = 15
        }

        update {
          # The maximum number of pages to update in one batch.
          max-batch-size = 100

          # The maximum amount of time (after buffering the first batch element) to wait for more batch elements before sending the batch to the database.
          max-batch-delay = "350 milliseconds"
        }
      }
    }
  }

  # Settings related to the Robots Exclusion Protocol (RFC 9309) and the crawl delay.
  robots {
    # Product tokens that identify the rules that are obeyed by this crawler.
    # While parsing a robots.txt file, the parser looks for rule groups with a user-agent line (e.g. "user-agent: MyBot") that matches any product token (e.g. "MyBot") from this array. The rules from all matched rule groups are merged.
    # If no match is found, the rules identified by the wildcard user-agent line (i.e. "user-agent: *") are used.
    user-agents = []

    # The default crawl delay (i.e. time between requests to the same host) to be used if no crawl delay is specified by the robots.txt file.
    default-crawl-delay = "1 second"

    # The minimum accepted crawl delay.
    # If a robots.txt file specifies a shorter crawl delay, the delay from this field is used.
    min-crawl-delay = "0 seconds"

    # The maximum accepted crawl delay.
    # If a robots.txt file specifies a longer crawl delay, the delay from this field is used.
    max-crawl-delay = "20 seconds"

    # Cache lifetime for rules from a robots.txt file.
    # When the cached rules expire, the robots.txt file is fetched again (if the host is still being crawled).
    # The Robots Exclusion Protocol specifies a maximum value of 24 hours.
    valid-rules-lifetime = "24 hours"

    # Cache lifetime for the default rules that are used if a robots.txt file is unavailable as defined by the Robots Exclusion Protocol (e.g. if the server responds with a 4xx status code).
    unavailable-rules-lifetime = "24 hours"

    # Cache lifetime for the default rules that are used if a robots.txt file is unreachable as defined by the Robots Exclusion Protocol (e.g. if the server responds with a 5xx status code).
    unreachable-rules-lifetime = "1 hour"

    # Settings related to fetching robots.txt files.
    fetching {
      # The maximum accepted content length (in bytes) per robots.txt file.
      # If a robots.txt file exceeds this length, it is truncated.
      # The Robots Exclusion Protocol specifies a minimum value of 500 kibibytes.
      max-content-length = "500 KiB"

      # The maximum number of consecutive redirections per robots.txt file.
      # If the robots.txt file is not reached after this number of redirections, the file is treated as unavailable.
      # The Robots Exclusion Protocol specifies a minimum value of five.
      max-redirects = 5

      # The maximum number of robots.txt files that can be fetched simultaneously.
      max-concurrent-files = 5

      # The bandwidth budget per robots.txt file.
      bandwidth-budget-per-file = "200 kB"
    }
  }
}
